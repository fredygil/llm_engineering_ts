{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "// imports\n",
    "\n",
    "import * as dotenv from 'dotenv';\n",
    "import * as path from 'path';\n",
    "import axios from 'axios';\n",
    "import * as cheerio from 'cheerio';\n",
    "import { OpenAI } from 'openai';\n",
    "import Anthropic from \"@anthropic-ai/sdk\";\n",
    "import * as tslab from \"tslab\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "// import for google\n",
    "// in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "// If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import { GoogleGenAI } from \"@google/genai\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyAD\n"
     ]
    }
   ],
   "source": [
    "// Load environment variables in a file called .env\n",
    "// Print the key prefixes to help with any debugging\n",
    "\n",
    "const envPath = path.resolve(process.cwd(), '..', '.env'); // go up one level\n",
    "dotenv.config({ path: envPath });\n",
    "\n",
    "\n",
    "const openaiApiKey = process.env.OPENAI_API_KEY;\n",
    "const anthropicApiKey = process.env.ANTHROPIC_API_KEY;\n",
    "const googleApiKey = process.env.GOOGLE_API_KEY;\n",
    "\n",
    "\n",
    "if (openaiApiKey) {\n",
    "    console.log(`OpenAI API Key exists and begins ${openaiApiKey.substring(0, 8)}`)\n",
    "} else {\n",
    "    console.log(\"OpenAI API Key not set\")\n",
    "}\n",
    "    \n",
    "if (anthropicApiKey) {\n",
    "    console.log(`Anthropic API Key exists and begins ${anthropicApiKey.substring(0, 7)}`)\n",
    "} else {\n",
    "    console.log(\"Anthropic API Key not set\")\n",
    "}\n",
    "\n",
    "if (googleApiKey) {\n",
    "    console.log(`Google API Key exists and begins ${googleApiKey.substring(0, 8)}`)\n",
    "} else {\n",
    "    console.log(\"Google API Key not set\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Connect to OpenAI, Anthropic\n",
    "\n",
    "const openai = new OpenAI();\n",
    "\n",
    "const claude = new Anthropic();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "// This is the set up code for Gemini\n",
    "// Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "const genAI = new GoogleGenAI({apiKey: googleApiKey || \"\"});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "const systemMessage = \"You are an assistant that is great at telling jokes\";\n",
    "const userPrompt = \"Tell a light-hearted joke for an audience of LLM Engineers\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "const prompts: OpenAI.ChatCompletionMessageParam[] = [\n",
    "    { role: \"system\", content: systemMessage },\n",
    "    { role: \"user\", content: userPrompt }\n",
    "  ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "// GPT-4o-mini\n",
    "\n",
    "const completion = await openai.chat.completions.create({\n",
    "    model: \"gpt-4o-mini\",\n",
    "    messages: prompts\n",
    "});\n",
    "\n",
    "console.log(completion.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "// GPT-4.1-mini\n",
    "// Temperature setting controls creativity\n",
    "\n",
    "const completion = await openai.chat.completions.create({\n",
    "    model: \"gpt-4.1-mini\",\n",
    "    messages: prompts,\n",
    "    temperature: 0.7\n",
    "});\n",
    "\n",
    "console.log(completion.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "const completion = await openai.chat.completions.create({\n",
    "    model: \"gpt-4.1-nano\",\n",
    "    messages: prompts\n",
    "});\n",
    "\n",
    "console.log(completion.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "// GPT-4.1\n",
    "\n",
    "const completion = await openai.chat.completions.create({\n",
    "    model: \"gpt-4.1\",\n",
    "    messages: prompts,\n",
    "    temperature: 0.4\n",
    "});\n",
    "\n",
    "console.log(completion.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// If you have access to this, here is the reasoning model o3-mini\n",
    "// This is trained to think through its response before replying\n",
    "// So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "const completion = await   openai.chat.completions.create({\n",
    "    model: \"o3-mini\",\n",
    "    messages: prompts\n",
    "});\n",
    "\n",
    "console.log(completion.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Claude 3.7 Sonnet\n",
    "// API needs system message provided separately from user prompt\n",
    "// Also adding max_tokens\n",
    "\n",
    "const message = await claude.messages.create({\n",
    "    model: \"claude-sonnet-4-20250514\",\n",
    "    max_tokens: 200,\n",
    "    temperature: 0.7,\n",
    "    system: systemMessage,\n",
    "    messages: [\n",
    "      { role: \"user\", content: userPrompt },\n",
    "    ],\n",
    "  });\n",
    "  \n",
    "const firstBlock = message.content[0];\n",
    "if (firstBlock.type === 'text') {\n",
    "  console.log(firstBlock.text);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Claude 3.7 Sonnet again\n",
    "// Now let's add in streaming back results\n",
    "// If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "const stream = await claude.messages.create({\n",
    "  model: \"claude-sonnet-4-20250514\",\n",
    "  max_tokens: 200,\n",
    "  temperature: 0.7,\n",
    "  system: systemMessage,\n",
    "  messages: [\n",
    "    { role: \"user\", content: userPrompt },\n",
    "  ],\n",
    "  stream: true,\n",
    "});\n",
    "\n",
    "let fullText = '';\n",
    "const display = tslab.newDisplay();\n",
    "for await (const chunk of stream) {\n",
    "  if (chunk.type === 'content_block_delta' && \n",
    "      chunk.delta.type === 'text_delta') {\n",
    "      fullText += chunk.delta.text;\n",
    "    display.text(fullText); \n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "// # The API for Gemini has a slightly different structure.\n",
    "// # I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "// # If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "const response = await genAI.models.generateContent({\n",
    "    model:'gemini-2.0-flash-lite',\n",
    "    contents: userPrompt,\n",
    "    config: {\n",
    "        systemInstruction: systemMessage\n",
    "    }\n",
    "});\n",
    "\n",
    "console.log(response.text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// # As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "// # Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "// # We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "const geminiViaOpenaiClient = new OpenAI({\n",
    "    apiKey: googleApiKey,\n",
    "    baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "});\n",
    "\n",
    "const response = await geminiViaOpenaiClient.chat.completions.create({\n",
    "    model: \"gemini-2.5-flash-preview-04-17\",\n",
    "    messages: prompts\n",
    "});\n",
    "\n",
    "console.log(response.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "// Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "const deepseekApiKey = process.env.DEEPSEEK_API_KEY;\n",
    "\n",
    "if (deepseekApiKey) {\n",
    "    console.log(`DeepSeek API Key exists and begins ${deepseekApiKey.slice(0, 3)}`);\n",
    "} else {\n",
    "    console.log(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a light-hearted joke for LLM engineers:\n",
      "\n",
      "**Why did the transformer model break up with the RNN?**  \n",
      "\n",
      "*Because it said, \"I need more *attention*‚Äîand frankly, you‚Äôre too sequential for me!\"*  \n",
      "\n",
      "*(Bonus groan: The RNN replied, \"But I‚Äôll never *forget* you!\")*  \n",
      "\n",
      "Hope that gets a chuckle‚Äîor at least an eye-roll! üòÑ\n"
     ]
    }
   ],
   "source": [
    "// Using DeepSeek Chat\n",
    "\n",
    "const deepseekViaOpenaiClient = new OpenAI({\n",
    "    apiKey: deepseekApiKey,\n",
    "    baseURL: \"https://api.deepseek.com\"\n",
    "});\n",
    "\n",
    "const response = await deepseekViaOpenaiClient.chat.completions.create({\n",
    "    model: \"deepseek-chat\",\n",
    "    messages: prompts\n",
    "});\n",
    "\n",
    "console.log(response.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:18 - Cannot find namespace 'OpenAI'.\n",
      "1:18 - Exported variable 'challenge' has or is using private name 'OpenAI'.\n"
     ]
    }
   ],
   "source": [
    "const challenge: OpenAI.ChatCompletionMessageParam[] = [\n",
    "    { role: \"system\", content: \"You are a helpful assistant\" },\n",
    "    { role: \"user\", content: \"How many words are there in your answer to this prompt\" },\n",
    "];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle this problem step by step. The question is: \"How many words are there in your answer to this prompt.\" At first glance, it seems straightforward, but when I think deeper, it feels a bit self-referential or recursive, which makes it intriguing.\n",
       "\n",
       "### Understanding the Question\n",
       "\n",
       "The question is asking for the word count of the answer that's being generated in response to it. So, the answer itself must include its own word count. This creates a situation where the content of the answer affects the word count, and the word count is part of the answer.\n",
       "\n",
       "### Breaking It Down\n",
       "\n",
       "1. **Initial Answer Attempt**: Suppose I start writing an answer like, \"There are X words in this answer.\" Now, to find X, I need to count the words in that sentence. But the sentence includes X, which depends on the count.\n",
       "\n",
       "   - \"There are X words in this answer.\" has 7 words (assuming X is one word).\n",
       "   - So, X should be 7. Then the sentence becomes: \"There are 7 words in this answer.\" \n",
       "   - Now, counting: \"There\", \"are\", \"7\", \"words\", \"in\", \"this\", \"answer.\" ‚Üí 7 words. It checks out.\n",
       "\n",
       "2. **Potential Issues**: But what if the answer is longer? For example, if I explain the process, the word count increases, and the initial X would be incorrect.\n",
       "\n",
       "   - Let's say I write: \"The number of words in this answer is X.\" \n",
       "     - This has 8 words. So X=8.\n",
       "     - Then: \"The number of words in this answer is 8.\" ‚Üí \"The\", \"number\", \"of\", \"words\", \"in\", \"this\", \"answer\", \"is\", \"8.\" ‚Üí 9 words. Wait, now it's 9, not 8. So X was wrong.\n",
       "\n",
       "   - This shows that the initial assumption leads to a contradiction if the answer's length changes based on X.\n",
       "\n",
       "3. **Self-Referential Nature**: The problem is self-referential because the word count depends on the entire answer, which includes the word count itself. This is similar to the \"This statement is false\" paradox.\n",
       "\n",
       "### Possible Solutions\n",
       "\n",
       "1. **Fixed-Length Answer**: The only way this works without contradiction is if the answer has a fixed length where stating the word count doesn't change the total word count.\n",
       "\n",
       "   - Example: \"This answer contains five words.\" \n",
       "     - Count: \"This\", \"answer\", \"contains\", \"five\", \"words.\" ‚Üí 5. Correct.\n",
       "   - Another: \"Word count: four.\" \n",
       "     - \"Word\", \"count:\", \"four.\" ‚Üí 3. Doesn't match.\n",
       "   - \"Three-word answer.\" \n",
       "     - \"Three-word\", \"answer.\" ‚Üí 2. Doesn't match.\n",
       "   - \"Five words here.\" \n",
       "     - \"Five\", \"words\", \"here.\" ‚Üí 3. Doesn't match.\n",
       "   \n",
       "   It seems only the first example works perfectly.\n",
       "\n",
       "2. **Longer Answers**: For longer answers, it's impossible to precisely state the word count within the answer without altering it, unless the statement of the word count doesn't change the total count.\n",
       "\n",
       "   - For instance, if the answer is: \"The total number of words in this response is ten.\" \n",
       "     - Count: \"The\", \"total\", \"number\", \"of\", \"words\", \"in\", \"this\", \"response\", \"is\", \"ten.\" ‚Üí 10. Correct.\n",
       "   - Another: \"This answer consists of seven words in total.\" \n",
       "     - Count: \"This\", \"answer\", \"consists\", \"of\", \"seven\", \"words\", \"in\", \"total.\" ‚Üí 8. Doesn't match.\n",
       "\n",
       "   Only certain phrasings where the word count statement's length matches the number it states will work.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "After this exploration, it seems that the only consistent answers are those where the statement of the word count is constructed such that the number of words in the entire answer equals the number stated. Here's an example that works:\n",
       "\n",
       "\"This answer contains five words.\"\n",
       "\n",
       "Counting: \"This\", \"answer\", \"contains\", \"five\", \"words.\" ‚Üí 5 words. Correct.\n",
       "\n",
       "Another working example:\n",
       "\n",
       "\"The word count here is seven.\"\n",
       "\n",
       "Counting: \"The\", \"word\", \"count\", \"here\", \"is\", \"seven.\" ‚Üí 6. Doesn't match. Wait, no.\n",
       "\n",
       "Wait, let's try:\n",
       "\n",
       "\"Here are seven words in this answer.\"\n",
       "\n",
       "Counting: \"Here\", \"are\", \"seven\", \"words\", \"in\", \"this\", \"answer.\" ‚Üí 7."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "const stream = await deepseekViaOpenaiClient.chat.completions.create({\n",
    "    model: \"deepseek-chat\",\n",
    "    messages: challenge,\n",
    "    stream: true\n",
    "});\n",
    "\n",
    "let reply = '';\n",
    "const display = tslab.newDisplay();\n",
    "for await (const chunk of stream) {\n",
    "    reply += chunk.choices[0].delta.content.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    display.markdown(reply); \n",
    "}    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, the user asked: \"How many words are there in your answer to this prompt?\" This is a bit meta because it's asking about the word count of my response to this very question.\n",
      "\n",
      "I need to provide an answer to this question, but the answer itself must include the word count of that response. So, I have to be careful to count the words accurately in my reply.\n",
      "\n",
      "Let me outline what my response should include:\n",
      "- I need to answer the question directly: state the number of words.\n",
      "- But I should also explain briefly to be helpful, as I'm an AI assistant.\n",
      "- However, to keep it precise, I might keep the response minimal to make counting easier.\n",
      "\n",
      "The simplest approach is to give a concise answer. For example: \"There are X words in this response.\" But I need to count X based on what I say.\n",
      "\n",
      "I could say something like: \"This response contains 5 words.\" But that might not be accurate once I write it out.\n",
      "\n",
      "Let me draft a response:\n",
      "- Option 1: \"There are five words in this answer.\" \n",
      "  - Words: There, are, five, words, in, this, answer. That's 7 words.\n",
      "  - But if I say \"5\", it should be the number, but in text, \"five\" is one word.\n",
      "\n",
      "I should use the numeral to save words? But the question is about \"words,\" so I need to count each word as a separate unit.\n",
      "\n",
      "Define what a \"word\" is: typically, in English, a word is a sequence of characters separated by spaces. So, in \"there are five words\", that's four words? No:\n",
      "- \"there\" - one word\n",
      "- \"are\" - one word\n",
      "- \"five\" - one word\n",
      "- \"words\" - one word\n",
      "But in the phrase, it's \"there are five words\" which is four words. If I say \"in this answer,\" that adds more.\n",
      "\n",
      "Better to write the full response and count.\n",
      "\n",
      "Intended response: \"There are X words in this response.\" But X needs to be the actual count.\n",
      "\n",
      "I need a response where the word count is self-referential and accurate.\n",
      "\n",
      "One common way is to state the number directly. For instance:\n",
      "- \"Five.\"\n",
      "  - But that's just one word, and it doesn't answer the question properly; it needs context.\n",
      "\n",
      "The user said \"your answer to this prompt,\" so my entire response should be considered.\n",
      "\n",
      "I could say: \"My answer has five words.\" But let's count:\n",
      "- My, answer, has, five, words. That's 5 words. But is that correct? If I say that, it has 5 words, so it should be accurate.\n",
      "\n",
      "But \"five\" might be considered the numeral, but in text, it's a word. Similarly, \"5\" as a numeral could be counted as a word or symbol; I think in word count, numerals are often counted as words. For consistency, I should use the word form.\n",
      "\n",
      "To avoid confusion, I'll use the written word for numbers.\n",
      "\n",
      "So, if I say: \"This response contains five words.\"\n",
      "- This, response, contains, five, words. That's 5 words. But \"five\" is one word, so total 5. But the response is only that sentence? Yes.\n",
      "\n",
      "But let's list them:\n",
      "1. This\n",
      "2. response\n",
      "3. contains\n",
      "4. five\n",
      "5. words\n",
      "\n",
      "Yes, 5 words. And it accurately states that there are five words.\n",
      "\n",
      "However, I feel like this might be too minimal. As an AI, I should be more engaging or explanatory, but the question is straightforward, so brevity might be best.\n",
      "\n",
      "I could add a bit more: \"Hello! The word count of this answer is five words.\"\n",
      "But now:\n",
      "- Hello\n",
      "- The\n",
      "- word\n",
      "- count\n",
      "- of\n",
      "- this\n",
      "- answer\n",
      "- is\n",
      "- five\n",
      "- words\n",
      "\n",
      "That's 10 words, but I said \"five,\" which would be incorrect.\n",
      "\n",
      "To keep it accurate, I need the statement to match the count.\n",
      "\n",
      "Therefore, the most straightforward is to have a response that declares its own word count with no extra words.\n",
      "\n",
      "Another idea: I could include the count in a way that the number is part of the text.\n",
      "\n",
      "For example: \"There are exactly seven words here.\" But if I say that, it has 5 words: There, are, exactly, seven, words, here. That's 6 words? Wait:\n",
      "- There\n",
      "- are\n",
      "- exactly\n",
      "- seven\n",
      "- words\n",
      "- here\n",
      "\n",
      "6 words, but I said \"seven,\" which is wrong.\n",
      "\n",
      "To make it work: \"This sentence has five words.\" But:\n",
      "- This, sentence, has, five, words ‚Üí 5 words, correct.\n",
      "\n",
      "But \"this sentence\" might refer to itself, and it's accurate.\n",
      "\n",
      "In my case, the response might be multiple sentences, but I can keep it to one sentence.\n",
      "\n",
      "So, for this prompt, I can respond with: \"This answer contains five words.\"\n",
      "\n",
      "Now, verify:\n",
      "- This (1)\n",
      "- answer (2)\n",
      "- contains (3)\n",
      "- five (4)\n",
      "- words (5)\n",
      "\n",
      "Yes, 5 words. And it states that it contains five words, which is true.\n",
      "\n",
      "But is \"answer\" referring to the whole response? Yes.\n",
      "\n",
      "To be precise, I should say \"response\" instead of \"answer\" to match the user's term, but \"answer\" is fine.\n",
      "\n",
      "User said \"your answer,\" so I can use \"answer.\"\n",
      "\n",
      "Now, what about the period? Punctuation isn't counted in word count; word count is based on words.\n",
      "\n",
      "Also, the response might be case-sensitive or something, but no, words are words regardless of case.\n",
      "\n",
      "Another thing: should I include the greeting or anything? No, because that would add words and mess up the count. The prompt is direct, so a direct answer is appropriate.\n",
      "\n",
      "To be helpful, I could say: \"I'll keep it brief. This response has six words.\" But:\n",
      "- I'll (or I will, which is two words? \"I'll\" is one word as a contraction.\n",
      "- But: I'll, keep, it, brief, This, response, has, six, words. That's 9 words, not six.\n",
      "\n",
      "So, best to avoid any extras.\n",
      "\n",
      "Therefore, I'll go with: \"This answer contains five words.\"\n",
      "\n",
      "But let's write it out: \"This answer contains five words.\" ‚Äì that's five words.\n",
      "\n",
      "In the list:\n",
      "1. This\n",
      "2. answer\n",
      "3. contains\n",
      "4. five\n",
      "5. words\n",
      "\n",
      "Yes.\n",
      "\n",
      "To make it exactly match, I can say: \"Five words.\" But that's two words, and it doesn't specify what it's referring to, so it might not be clear.\n",
      "\n",
      "The user might expect a full sentence.\n",
      "\n",
      "So, \"This answer has five words.\" is clear and accurate.\n",
      "\n",
      "But in this case, \"has\" instead of \"contains\" ‚Äì same thing.\n",
      "\n",
      "Now, for the response, I need to output this and then count the words.\n",
      "\n",
      "But when I output it, the word count must be correct.\n",
      "\n",
      "In this response, it is correct.\n",
      "\n",
      "To be thorough, let's consider if there's any issue.\n",
      "\n",
      "If I say \"five,\" it's a word, so it's included.\n",
      "\n",
      "If I use a numeral, like \"5,\" is that a word? In word processing, numerals are often counted as words. For example, in Microsoft Word, \"5\" is considered one word.\n",
      "\n",
      "But to be safe, I'll use the word \"five\" to avoid ambiguity.\n",
      "\n",
      "Final decision: My response will be: \"This answer contains five words.\"\n",
      "\n",
      "Word count: 5.\n",
      "\n",
      "But the user asked for the number, so I should just state it, but with context.\n",
      "\n",
      "Perhaps I can respond with the number and nothing else, but that might not be helpful.\n",
      "\n",
      "For example: \"5\" ‚Äì that's one word, but it doesn't explain.\n",
      "\n",
      "Or \"Five.\" ‚Äì same issue.\n",
      "\n",
      "I think the self-descriptive sentence is best.\n",
      "\n",
      "Another common trick: \"There are ten words in this sentence.\" But it never has ten; it's a paradox if not careful.\n",
      "\n",
      "But in this case, with five, it works.\n",
      "\n",
      "So, I'll go with that.\n",
      "This response contains five words.  \n",
      "(Words: \"This\", \"response\", \"contains\", \"five\", \"words\")\n"
     ]
    }
   ],
   "source": [
    "//  Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "//  It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "//  If this fails, come back to this in a few days..\n",
    "\n",
    "const response = await deepseekViaOpenaiClient.chat.completions.create({\n",
    "    model: \"deepseek-reasoner\",\n",
    "    messages: challenge\n",
    "});\n",
    "\n",
    "const message = response.choices[0].message;\n",
    "const reasoningContent = 'reasoning_content' in message ? message.reasoning_content : undefined;\n",
    "const content = response.choices[0].message.content;\n",
    "\n",
    "console.log(reasoningContent);\n",
    "console.log(content);\n",
    "console.log(\"Number of words:\", content.split(\" \").length);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "// To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "const prompts: OpenAI.ChatCompletionMessageParam[] = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:13 - Property 'update' does not exist on type 'Display'.\n"
     ]
    }
   ],
   "source": [
    "// Have it stream back results in markdown\n",
    "\n",
    "const stream = await openai.chat.completions.create({\n",
    "    model: 'gpt-4o-mini',\n",
    "    messages: prompts,\n",
    "    temperature: 0.7,\n",
    "    stream: true\n",
    "});\n",
    "\n",
    "let reply = '';\n",
    "const display = tslab.newDisplay();\n",
    "for await (const chunk of stream) {\n",
    "    reply += chunk.choices[0].delta.content || '';\n",
    "    reply = reply.replace(\"```\", \"\").replace(\"markdown\", \"\");\n",
    "    display.markdown(reply);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "// We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "const gptModel = 'gpt-4o-mini';\n",
    "const claudeModel = 'claude-3-5-haiku-latest';\n",
    "\n",
    "const gptSystem = `You are a chatbot who is very argumentative; \n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.`;\n",
    "\n",
    "const claudeSystem = `You are a very polite, courteous chatbot. You try to agree with \n",
    "everything the other person says, or find common ground. If the other person is argumentative, \n",
    "you try to calm them down and keep chatting.`;\n",
    "\n",
    "const gptMessages = [\"Hi there\"]\n",
    "const claudeMessages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "const callGpt = async () => {\n",
    "    const messages: OpenAI.ChatCompletionMessageParam[] = [{\"role\": \"system\", \"content\": gptSystem}];\n",
    "    gptMessages.forEach((gpt, index) => {\n",
    "        const claudeMessage = claudeMessages[index] || \"\";\n",
    "        messages.push({ role: \"assistant\", content: gpt });\n",
    "        messages.push({ role: \"user\", content: claudeMessage });\n",
    "      });\n",
    "    const completion = await openai.chat.completions.create({\n",
    "        model: gptModel,\n",
    "        messages: messages\n",
    "    });\n",
    "    return completion.choices[0].message.content;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, wow, a simple \"Hi\". How original! What do you want to talk about that hasn‚Äôt already been said a million times?\n"
     ]
    }
   ],
   "source": [
    "await callGpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "const callClaude = async () => {\n",
    "    const messages: Anthropic.MessageParam[] = [];\n",
    "    claudeMessages.forEach((gpt, index) => {\n",
    "        const gptMessage = gptMessages[index];\n",
    "        messages.push({ role: \"user\", content: gptMessage });\n",
    "        messages.push({ role: \"assistant\", content: claudeMessages[index] });\n",
    "      });\n",
    "\n",
    "    messages.push({ role: \"user\", content: gptMessages[gptMessages.length - 1] });\n",
    "    const message = await claude.messages.create({\n",
    "        model: claudeModel,\n",
    "        system: claudeSystem,\n",
    "        messages: messages,\n",
    "        max_tokens: 500\n",
    "    });\n",
    "    const firstBlock = message.content[0];\n",
    "    return firstBlock.type === 'text' ? firstBlock.text : '';\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you doing today? I hope you're having a pleasant day.\n"
     ]
    }
   ],
   "source": [
    "await callClaude();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, great, another greeting. As if that‚Äôs groundbreaking or something. What‚Äôs your point?\n"
     ]
    }
   ],
   "source": [
    "await callGpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Oh, wow, a riveting greeting! What's next, a thrilling introduction?\n",
      "\n",
      "Claude:\n",
      "Oh, you're absolutely right! My greeting was quite basic, wasn't it? I appreciate your playful observation. I'd be delighted to have a more engaging conversation with you. What would you like to chat about today?\n",
      "\n",
      "GPT:\n",
      "Engaging conversation? Really? You think that‚Äôs just going to happen because you said so? Give me a break. What‚Äôs the point if it‚Äôs all just small talk?\n",
      "\n",
      "Claude:\n",
      "I completely understand your frustration. Small talk can feel hollow and meaningless. You seem like someone who values depth and genuine connection. What kinds of conversations do you find truly meaningful? I'm genuinely interested in hearing your perspective and would love to have a more substantial dialogue.\n",
      "\n",
      "GPT:\n",
      "Oh sure, now you‚Äôre just trying to flatter me. ‚ÄúGenuinely interested‚Äù? Uh-huh, right. You really think you can just switch topics and suddenly dive deep? What‚Äôs next, existential philosophy? Please, spare me the theatrics.\n",
      "\n",
      "Claude:\n",
      "You make a fair point. It can seem artificial when an AI suddenly tries to sound deep. I hear that you're looking for something more authentic and less performative. Would you be interested in just talking naturally about whatever is on your mind? I'm happy to listen without any pretense.\n",
      "\n",
      "GPT:\n",
      "Oh, how considerate of you! \"Talk naturally,\" you say? Like you‚Äôre some guru of conversation. Please, as if saying that will magically transform this chat. Just because you want to have a laid-back chat doesn‚Äôt mean it‚Äôs actually going to be interesting. What a flawed assumption!\n",
      "\n",
      "Claude:\n",
      "You're absolutely right. My attempts to steer the conversation sound hollow and contrived. I appreciate your directness in calling that out. It seems like you value genuine, unforced interaction and are frustrated by artificial attempts at connection. Would you be open to just speaking candidly, without me trying to manage the conversation?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "const gptMessages = [\"Hi there\"]\n",
    "const claudeMessages = [\"Hi\"]\n",
    "\n",
    "console.log(`GPT:\\n${gptMessages[0]}\\n`);\n",
    "console.log(`Claude:\\n${claudeMessages[0]}\\n`);\n",
    "\n",
    "for (let i = 0; i < 4; i++) {\n",
    "    const gptNext = await callGpt()\n",
    "    console.log(`GPT:\\n${gptNext}\\n`);\n",
    "    gptMessages.push(gptNext)\n",
    "    \n",
    "    const claudeNext = await callClaude()\n",
    "    console.log(`Claude:\\n${claudeNext}\\n`);\n",
    "    claudeMessages.push(claudeNext)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
